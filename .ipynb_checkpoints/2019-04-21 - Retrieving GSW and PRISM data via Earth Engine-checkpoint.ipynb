{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for processing Global Surface Water data from raster format into water extent polygons and then computing time series of area within each polygon. It also retrieves meteorological forcings for the area within the polygons. The basic input require for this script is a series of latitude/longitude coordinates that delineate a rectangular area of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the computation will be done using the `ee` API for Google Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ee\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we'll specify a prefix for all of our files that will be downloaded. We'll also make an empty dict which will later be used to hold Earth Engine API request objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_name = 'test_2019_06_26_'\n",
    "tasks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running the next code cell, the Earth Engine API must be configured on your computer. The instructions for doing so can be found [here](https://developers.google.com/earth-engine/python_install_manual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define several functions that will be used to process the water data. Suppose that we have a `FeatureCollection` indicating where water can be found. `single_time_areas` is used to compute a per-water polygon mean value for a single timestep and then add that mean value as metadata to the basin features.\n",
    "\n",
    "`multiple_time_areas` then maps the above function over an entire `ImageCollection` such that we are eventually given metadata which contains time series for each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_time_areas(image,basins,reducer=ee.Reducer.mean,propname='mean',aggregate=True):\n",
    "    features   = image.reduceRegions(basins,reducer=reducer())\n",
    "    if aggregate:\n",
    "        return features.aggregate_array(propname)\n",
    "    else:\n",
    "        return features\n",
    "\n",
    "def multiple_time_areas(image_collection,basins,reducer=ee.Reducer.mean,propname='mean',aggregate=True):\n",
    "    new_images = image_collection.map(lambda x: ee.Image().set('zonal_statistics',single_time_areas(x,\n",
    "                                                                                                    basins,\n",
    "                                                                                                    reducer=reducer,\n",
    "                                                                                                    propname=propname,\n",
    "                                                                                                    aggregate=aggregate)))\n",
    "    return new_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we indicate our area of interest and also see how big it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Areal extent is 1632.0864555544824 square kilometers.\n"
     ]
    }
   ],
   "source": [
    "upper  = 48.9\n",
    "lower  = 48.5\n",
    "left   = -99.0\n",
    "right  = -98.5\n",
    "\n",
    "\n",
    "bounds = ee.Geometry.Polygon([[[left, lower], [left, upper], \n",
    "                               [ right,upper], [right,lower], \n",
    "                               [left,lower]]])\n",
    "\n",
    "area_m2 = bounds.area().getInfo()\n",
    "area_km2 = area_m2 / 1000000\n",
    "print('Areal extent is {0} square kilometers.'.format(area_km2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a `FeatureCollection` indicating the maximum extent of each water body. To do this, we get the `max_extent` layer which has a 1 for water ever occurring and 0 otherwise. However, if we try to apply raster-to-vector conversion on this, Earth Engine will create separate polygons for 1 and 0 and the 0-valued polygons will lead to errors. Instead, we want to mask out the `max_extent` layer such that it has only 1 and no-data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_extent = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('max_extent').clip(bounds)\n",
    "max_extent = max_extent.updateMask(max_extent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to filter out water polygons which are too large or too small, we can do this with the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_pixels = 5\n",
    "max_pixels = 10000\n",
    "\n",
    "basins = max_extent.reduceToVectors(maxPixels=1000000000)\n",
    "basins = basins.map(area_perimeter_ratio)\n",
    "basins = basins.filterMetadata('count','greater_than',min_pixels).filterMetadata('count','less_than',max_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the basins are too small to have non-null overlap with PRISM as PRISM cells are much larger than individual GSW pixels. To solve this issue, we buffer all of them by 500 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffered_basins = basins.map(lambda x: x.buffer(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many water polygons or basins we can identify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4233\n"
     ]
    }
   ],
   "source": [
    "tasks['basins'] = ee.batch.Export.table.toDrive(basins, description=run_name + 'basins', folder='Research')\n",
    "n_basins = basins.size().getInfo()\n",
    "print(n_basins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we have delineated individual water bodies, we want to calculate time series of inundation within each one. This code cell sends a request to Earth Engine to export tables with time series of monthly inundation fraction for each basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WATER_PRESENT_VALUE = 2\n",
    "WATER_ABSENT_VALUES = 1\n",
    "DATA_MISSING_VALUE  = 0\n",
    "\n",
    "gsw_monthly       = ee.ImageCollection('JRC/GSW1_0/MonthlyHistory')\n",
    "gsw_water_present = gsw_monthly.map(lambda x: x.eq(WATER_PRESENT_VALUE))\n",
    "gsw_missing_data  = gsw_monthly.map(lambda x: x.eq(DATA_MISSING_VALUE))\n",
    "\n",
    "output_water        = multiple_time_areas(gsw_water_present,basins)\n",
    "output_missing_data = multiple_time_areas(gsw_missing_data,basins)\n",
    "\n",
    "tasks['water'] = ee.batch.Export.table.toDrive(output_water, description=run_name + 'water_areas', folder='Research')\n",
    "tasks['missing_data'] = ee.batch.Export.table.toDrive(output_missing_data, description=run_name + 'missing_data', folder='Research')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also do the same thing with PRISM forcings. Here, I'm only looking at precipitation, temperature and vapor pressure deficit, though other variables exist as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_date = '1984-03-01'\n",
    "end_date   = '2015-10-31'\n",
    "band_names = ['ppt', 'tmean', 'vpdmax']\n",
    "prism = ee.ImageCollection('OREGONSTATE/PRISM/AN81m').filterDate(start_date,end_date)\n",
    "prism = prism.map(lambda x: x.clip(bounds))\n",
    "\n",
    "for band in band_names:\n",
    "    out = multiple_time_areas(prism.select(band),buffered_basins)\n",
    "    tasks[band] = ee.batch.Export.table.toDrive(out, description= run_name + band, folder='Research')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in obtaining land use / land change statistics, the NASS cropland data layer is an invaluable source of data. I will note that this is available at a different temporal resolution and so cannot be aligned with PRISM and GSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CDL_WATER_CODE_1 = 111\n",
    "CDL_WATER_CODE_2 = 83\n",
    "\n",
    "cdl = ee.ImageCollection('USDA/NASS/CDL').select('cropland')\n",
    "\n",
    "# Since we want the dominant land use around the water body, we need to make sure we aren't counting water\n",
    "# as one of the possible land uses.\n",
    "cdl_no_water = cdl.map(lambda x: x.updateMask(x.neq(CDL_WATER_CODE_1).And(x.neq(CDL_WATER_CODE_2))))\n",
    "cdl_out = multiple_time_areas(cdl_no_water,buffered_basins,reducer=ee.Reducer.mode,propname='mode')\n",
    "\n",
    "tasks['cdl'] = ee.batch.Export.table.toDrive(cdl_out, description=run_name + 'cdl', folder='Research')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code cells added API request objects to `tasks` but did not start them. Here, we start all of these requests at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basins\n",
      "water\n",
      "missing_data\n",
      "ppt\n",
      "tmean\n",
      "vpdmax\n",
      "cdl\n"
     ]
    }
   ],
   "source": [
    "for variable in tasks.keys():\n",
    "    print(variable)\n",
    "    tasks[variable].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tasks may take awhile to run. You can repeatedly run the cell below to check the status of the tasks. The tasks should say `COMPLETED` when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: basins, status: COMPLETED\n",
      "Variable: water, status: COMPLETED\n",
      "Variable: missing_data, status: COMPLETED\n",
      "Variable: ppt, status: RUNNING\n",
      "Variable: tmean, status: RUNNING\n",
      "Variable: vpdmax, status: RUNNING\n",
      "Variable: cdl, status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "for variable in tasks.keys():\n",
    "    print('Variable: {0}, status: {1}'.format(variable,tasks[variable].status()['state']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the data off of our Google Drive and into local storage. I personally use Insync, a 3rd party client which syncs Google Drive to a folder on my computer. You could also drag-and-drop the files manually from Google Drive. Here, I just indicate where Insync has saved my files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_directory = '/home/ckrapu/ckrapu@gmail.com/Research/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few code cells take the GEE export files and turn them into dataframes. There are numerous details from `pandas` and `geopandas` here and I'm not going to explain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "filename = run_name + 'basins.csv'\n",
    "basin_gdf = pd.read_csv(outputs_directory + filename)\n",
    "as_shape = [shape(json.loads(feature)) for feature in basin_gdf['.geo'].values]\n",
    "basin_gdf['.geo'] = as_shape\n",
    "basin_gdf = basin_gdf.set_geometry('.geo')\n",
    "basin_gdf.crs={'init':'epsg:4326'}\n",
    "dataframes['basins'] = basin_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code cells take the CSV-formatted files generated by Google Earth Engine and parses them. It takes a string-values column and turns that into a 2D array of values over basins and timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_formats = {'water_areas':'yyyy_mm', \n",
    "                 'missing_data':'yyyy_mm',\n",
    "                 'ppt': 'yyyymm',\n",
    "                 'tmean': 'yyyymm',\n",
    "                 'vpdmax':'yyyymm'}\n",
    "                 #'cdl': 'yyyy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = index_formats.keys()\n",
    "\n",
    "for variable in variables:\n",
    "    filename = run_name + variable + '.csv'\n",
    "    index_format = index_formats[variable]\n",
    "    \n",
    "    # Load in CSV generated by GEE and use the \n",
    "    # system:index column to generate a Pandas Index\n",
    "    df = pd.read_csv(outputs_directory + filename)\n",
    "    series = df.set_index(df['system:index'])['zonal_statistics']\n",
    "    n_basins = len([float(x) for x in series.iloc[0].replace('[','').replace(']','').split(',')])\n",
    "      \n",
    "     # Handle an edge case with the CDL indexing\n",
    "    if variable == 'cdl':\n",
    "        series = series.drop(labels=['2005b', '2007b'])\n",
    "    \n",
    "    n_timesteps = series.shape[0]\n",
    "    data = np.zeros([n_timesteps,n_basins])\n",
    "    \n",
    "    # For each of the rows, take the CSV's data and place it into the dataframe\n",
    "    for i in range(n_timesteps):\n",
    "        numeric_values = [float(x) for x in series.iloc[i].replace('[','').replace(']','').split(',')]\n",
    "        data[i,:] = numeric_values\n",
    "        \n",
    "    old_index = series.index \n",
    "    years  = series.index.map(lambda x: int(str(x)[0:4]))\n",
    "    \n",
    "    # The format of system:index depends on the dataset used in GEE\n",
    "    if index_format == 'yyyymm': \n",
    "        months = series.index.map(lambda x: int(str(x)[4::]))\n",
    "    elif index_format == 'yyyy_mm':\n",
    "        months = series.index.map(lambda x: int(str(x)[5::]))\n",
    "    elif index_format == 'yyyy':\n",
    "        months = [1] * years.shape[0]\n",
    "    \n",
    "    dates = [pd.Timestamp(year=years[i],month=months[i],day=1) for i in range(len(years))]\n",
    "    datetime_index = pd.to_datetime(dates)\n",
    "    dataframes[variable] = pd.DataFrame(index=datetime_index,data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_missing = dataframes['missing_data'] > 0.\n",
    "dataframes['water_masked']  = pd.DataFrame(index=dataframes['water_areas'].index,data=dataframes['water_areas'].values)\n",
    "dataframes['water_masked'][is_missing] = np.nan\n",
    "areas = dataframes['water_masked'].values * dataframes['basins']['count'].values[np.newaxis,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of all of this, you can save the tables in `dataframes` to disk. I'd recommend using `pandas` function `to_pickle` or `to_csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
